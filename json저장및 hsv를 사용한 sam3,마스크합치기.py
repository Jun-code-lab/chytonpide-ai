import cv2
import numpy as np
import io
import logging
import os
import torch
import warnings
import json
import time
from PIL import Image, ImageOps
from ultralytics import YOLO

# â˜… Hugging Face Transformers (SAM 3)
# ì‹¤í–‰ ì „: pip install git+https://github.com/huggingface/transformers.git
try:
    from transformers import Sam3Processor, Sam3Model
except ImportError:
    print("âŒ [ì˜¤ë¥˜] transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    print("ğŸ‘‰ í„°ë¯¸ë„ì— ë‹¤ìŒì„ ì…ë ¥í•˜ì„¸ìš”: pip install git+https://github.com/huggingface/transformers.git")
    raise

# ==========================================
# [ì„¤ì •] ë¡œê·¸ ë° ê²½ê³  ì„¤ì •
# ==========================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings("ignore")

# ==========================================
# [ì„¤ì •] ëª¨ë¸ ë° íŒŒì¼ ê²½ë¡œ
# ==========================================
# 1. YOLO ëª¨ë¸ (ì „ì²´ ì‹ë¬¼ íƒì§€ìš©)
DET_MODEL_PATH = r"runs\detect\det_exp1\weights\best.pt"
CLS_MODEL_PATH = r"runs\classify\test1\weights\best.pt"

# 2. SAM 3 ì„¤ì • (Transformers)
# ìë™ìœ¼ë¡œ facebook/sam3 ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
SAM3_MODEL_ID = "facebook/sam3"
SAM_TEXT_PROMPT = "leaf"  # â˜… í…ìŠ¤íŠ¸ë¡œ ì ì°¾ê¸°

# 3. í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ ê²½ë¡œ
TEST_IMAGE_PATH = r"C:\Users\sega0\Desktop\chytonpide-ai\predict_image\test6.jpg"

# 4. ê¸°íƒ€ ì„¤ì •
SCALE_REAL_DIAMETER_MM = 16.0
GREEN_HSV_LOWER = [35, 40, 40]
GREEN_HSV_UPPER = [85, 255, 255]


class Sam3TransformersAnalyzer:
    """
    [SAM 3 Transformers ë¶„ì„ê¸°]
    - YOLO: ë°”ì§ˆ ìœ„ì¹˜(Crop) ì°¾ê¸°
    - SAM 3: 'basil leaf' í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì ì •ë°€ ë¶„í• 
    """

    def __init__(self):
        logger.info("ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ‘‰ ì‚¬ìš© ì¥ì¹˜: {self.device}")

        try:
            # 1. YOLO ëª¨ë¸ ë¡œë”©
            self.det_model = YOLO(DET_MODEL_PATH)
            self.cls_model = YOLO(CLS_MODEL_PATH)
            logger.info("âœ… YOLO ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

            # 2. SAM 3 ëª¨ë¸ ë¡œë”© (Transformers)
            logger.info(f"â³ SAM 3 ëª¨ë¸({SAM3_MODEL_ID}) ë‹¤ìš´ë¡œë“œ ë° ë¡œë”© ì¤‘...")
            self.processor = Sam3Processor.from_pretrained(SAM3_MODEL_ID)
            self.sam_model = Sam3Model.from_pretrained(SAM3_MODEL_ID).to(self.device)
            logger.info(f"âœ… SAM 3 ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _determine_stage(self, leaf_count):
        if leaf_count <= 2:
            return "ğŸŒ± ë–¡ì ë‹¨ê³„", "ë–¡ìë§Œ ì¡´ì¬í•˜ê±°ë‚˜, ë³¸ì—½ì´ ë‚˜ì˜¤ê¸° ì§ì „ì…ë‹ˆë‹¤."
        elif 3 <= leaf_count <= 4:
            return "ğŸŒ¿ ë³¸ì—½ 2ë§¤", "ë³¸ì—½ì´ 1ìŒ(2ì¥) ì „ê°œëœ ìƒíƒœì…ë‹ˆë‹¤."
        elif 5 <= leaf_count <= 8:
            return "ğŸŒ¿ ë³¸ì—½ 4ë§¤ ~ 8ë§¤", "ë³¸ì—½ì´ 2ìŒì—ì„œ 4ìŒê¹Œì§€ í™œë°œíˆ ìë¼ëŠ” ì¤‘ì…ë‹ˆë‹¤."
        elif 9 <= leaf_count <= 10:
            return "ğŸŒ¿ ë³¸ì—½ 8ë§¤ ~ 10ë§¤", "ë³¸ì—½ ì„±ì¥ì´ ê±°ì˜ ì™„ë£Œë˜ì–´ ê°€ë©°, ê³§ ë¶„ì§€ê°€ ì˜ˆìƒë©ë‹ˆë‹¤."
        else:
            return "ğŸŒ³ ë¶„ì§€ ë°œìƒ", "ìì´ 10ë§¤ ì´ìƒì´ë©°, ê³ê°€ì§€(ë¶„ì§€)ê°€ ë°œë‹¬í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤."

    def _run_sam3_text(self, basil_crop_pil):
        """
        [SAM 3 í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë¡œì§]
        Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰
        """
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì…ë ¥
            inputs = self.processor(
                images=basil_crop_pil, 
                text=SAM_TEXT_PROMPT, 
                return_tensors="pt"
            ).to(self.device)

            # ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                outputs = self.sam_model(**inputs)

            # í›„ì²˜ë¦¬ (Instance Segmentation)
            # threshold: í™•ì‹ ë„, mask_threshold: ë§ˆìŠ¤í¬ ì´ì§„í™” ì„ê³„ê°’
            results = self.processor.post_process_instance_segmentation(
                outputs,
                threshold=0.4,       # ê°ë„ ì¡°ì ˆ (ë‚®ì„ìˆ˜ë¡ ë§ì´ ì°¾ìŒ)
                mask_threshold=0.5,
                target_sizes=inputs.get("original_sizes").tolist()
            )[0]

            # ê²°ê³¼ ì¶”ì¶œ
            masks = results['masks']  # (N, H, W) Tensor
            scores = results['scores'] # (N,) Tensor
            
            # í…ì„œë¥¼ ë„˜íŒŒì´ë¡œ ë³€í™˜
            masks_np = masks.cpu().numpy().astype(np.uint8)
            leaf_count = len(masks_np)
            
            logger.info(f"ğŸ” SAM 3ê°€ '{SAM_TEXT_PROMPT}'ë¡œ {leaf_count}ê°œì˜ ìì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

            # ì‹œê°í™”ë¥¼ ìœ„í•´ ë§ˆìŠ¤í¬ í•©ì¹˜ê¸°
            w, h = basil_crop_pil.size
            combined_mask = np.zeros((h, w), dtype=np.uint8)

            for mask in masks_np:
                # ê° ë§ˆìŠ¤í¬(0/1)ë¥¼ 255(í°ìƒ‰)ë¡œ ë³€í™˜í•˜ì—¬ í•©ì¹¨
                combined_mask = np.maximum(combined_mask, mask * 255)

            stage_name, message = self._determine_stage(leaf_count)

            return {
                "leaf_count": leaf_count,
                "stage": stage_name,
                "message": message,
                "mask": combined_mask,
                "raw_detected": leaf_count
            }

        except Exception as e:
            logger.error(f"âŒ SAM 3 ë¶„ì„ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _calculate_pla(self, basil_crop_bgr, mm_per_pixel):
        """ê¸°ì¡´ PLA ê³„ì‚°"""
        try:
            basil_hsv = cv2.cvtColor(basil_crop_bgr, cv2.COLOR_BGR2HSV)
            lower = np.array(GREEN_HSV_LOWER, dtype=np.uint8)
            upper = np.array(GREEN_HSV_UPPER, dtype=np.uint8)
            mask = cv2.inRange(basil_hsv, lower, upper)
            kernel = np.ones((3, 3), np.uint8)
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
            pixels = cv2.countNonZero(mask)
            area = pixels * (mm_per_pixel ** 2)
            return {"pla_mm2": round(area, 2), "green_pixels": pixels}
        except:
            return None

    def process_file(self, image_path):
        if not os.path.exists(image_path):
            return
        with open(image_path, "rb") as f:
            return self.process(f.read())

    def process(self, image_bytes):
        total_start_time = time.time()
        
        try:
            # 1. ì´ë¯¸ì§€ ë¡œë“œ
            origin_img_pil = Image.open(io.BytesIO(image_bytes))
            origin_img_pil = ImageOps.exif_transpose(origin_img_pil).convert("RGB")
            origin_img_bgr = cv2.cvtColor(np.array(origin_img_pil), cv2.COLOR_RGB2BGR)

            # 2. YOLO íƒì§€ (Crop & Scale)
            yolo_start = time.time()
            results = self.det_model(origin_img_pil, conf=0.15, verbose=False)
            
            mm_per_pixel = 0.1
            basil_crop_pil = None
            basil_crop_bgr = None
            
            if len(results) > 0:
                boxes = results[0].boxes
                cls_ids = boxes.cls.cpu().numpy().astype(int)
                for i, cls_id in enumerate(cls_ids):
                    x1, y1, x2, y2 = map(int, boxes[i].xyxy[0])
                    if cls_id == 1: # Scale
                        d = max(x2 - x1, y2 - y1)
                        mm_per_pixel = SCALE_REAL_DIAMETER_MM / d
                    elif cls_id == 0: # Basil
                        basil_crop_bgr = origin_img_bgr[y1:y2, x1:x2]
                        basil_crop_pil = Image.fromarray(cv2.cvtColor(basil_crop_bgr, cv2.COLOR_BGR2RGB))
                        logger.info(f"ğŸŒ¿ ë°”ì§ˆ ë°œê²¬! í¬ê¸°: {basil_crop_pil.size}")

            if basil_crop_pil is None:
                return {"status": "error", "message": "ë°”ì§ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

            # 3. SAM 3 í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë¶„ì„
            growth_info = self._run_sam3_text(basil_crop_pil)

            # 4. PLA ë° ê±´ê°• ë¶„ì„
            pla_result = self._calculate_pla(basil_crop_bgr, mm_per_pixel)
            
            cls_res = self.cls_model(basil_crop_pil, verbose=False)[0]
            health = cls_res.names[cls_res.probs.top1]
            conf = float(cls_res.probs.top1conf) * 100

            # 5. ì‹œê°í™” ì €ì¥
            self._save_visualization(basil_crop_bgr, growth_info)

            # JSONìš© ë°ì´í„°
            growth_data_json = growth_info.copy() if growth_info else None
            if growth_data_json and 'mask' in growth_data_json:
                del growth_data_json['mask']

            total_duration = time.time() - total_start_time
            logger.info(f"âš¡ ì‹¤í–‰ ì™„ë£Œ: {total_duration:.2f}ì´ˆ")

            return {
                "status": "success",
                "data": {
                    "health": {"status": health, "confidence": f"{conf:.2f}%"},
                    "pla": pla_result,
                    "growth": growth_data_json,
                    "execution_time": round(total_duration, 2)
                }
            }

        except Exception as e:
            logger.error(f"ì˜¤ë¥˜: {e}", exc_info=True)
            return {"status": "error", "message": str(e)}

    def _save_visualization(self, crop_img, growth_info):
        try:
            if growth_info and growth_info.get('mask') is not None:
                mask = growth_info['mask']
                if mask.shape[:2] != crop_img.shape[:2]:
                    mask = cv2.resize(mask, (crop_img.shape[1], crop_img.shape[0]), interpolation=cv2.INTER_NEAREST)
                
                color_mask = np.zeros_like(crop_img)
                color_mask[mask > 0] = [0, 255, 0]
                overlay = cv2.addWeighted(crop_img, 0.7, color_mask, 0.3, 0)
                
                txt = f"{growth_info['stage']} (Leaves: {growth_info['leaf_count']})"
                cv2.putText(overlay, txt, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                
                cv2.imwrite("result_sam3_transformers.jpg", overlay)
                logger.info("ğŸ’¾ ê²°ê³¼ ì €ì¥: result_sam3_transformers.jpg")
        except:
            pass

if __name__ == "__main__":
    analyzer = Sam3TransformersAnalyzer()
    print(f"\nğŸš€ SAM 3 (Transformers) ë¶„ì„ ì‹œì‘: {TEST_IMAGE_PATH}")
    
    final_result = analyzer.process_file(TEST_IMAGE_PATH)
    
    if final_result:
        json_filename = "result.json"
        with open(json_filename, "w", encoding="utf-8") as f:
            json.dump(final_result, f, ensure_ascii=False, indent=4)
        print("\nğŸ“Š [ë¶„ì„ ê²°ê³¼]")
        print(json.dumps(final_result, ensure_ascii=False, indent=4))
    else:
        print("âŒ ë¶„ì„ ì‹¤íŒ¨") 